# Language Detection using XLM-RoBERTa

This project is about detecting the language of a given text using a fine-tuned XLM-RoBERTa transformer model. The model was trained and evaluated on a multilingual corpus consisting of texts from various languages such as English, Spanish, German, French, Portuguese, and Italian.

## Usage

To use the language detection model, simply input a text string and the model will return the predicted language label for that text.

## Dataset

The dataset used for training and evaluation of the model consists of texts from various languages, obtained from publicly available sources.

## Models

The XLM-RoBERTa transformer model was fine-tuned using the TensorFlow and Keras libraries. The model achieved an f1-score of over 95% for all the languages in the dataset.

## Future Work

Future work could involve expanding the dataset to include more languages and dialects, as well as exploring different transformer models and techniques for language detection.
